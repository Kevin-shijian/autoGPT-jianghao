{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgXbuQQpC3+Cu7TnyzRHui",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acl-jianghao/Langchain-Scripts/blob/main/04_conversation_history_cache.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "conversation"
      ],
      "metadata": {
        "id": "Ik3mj6soQdf6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {
        "id": "uRQhHK-MPvA6"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqq OpenAI langchain tiktoken colab-env pymupdf pinecone-client\n",
        "!pip install -Uqq ndg-httpsclient pyopenssl pyasn1 colab-env pymupdf chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import colab_env"
      ],
      "metadata": {
        "id": "XiD7pfQWQbZG"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://progforperf.github.io/Expert_C_Programming.pdf -O /content/gdrive/MyDrive/data/pdf/c.pdf\n",
        "!wget https://www.halvorsen.blog/documents/programming/python/resources/Python%20Programming.pdf -O /content/gdrive/MyDrive/data/pdf/python.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r87C_EhbYXMD",
        "outputId": "7a208768-2a2f-46fc-f465-06e0d6757e32"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-25 07:44:38--  https://progforperf.github.io/Expert_C_Programming.pdf\n",
            "Resolving progforperf.github.io (progforperf.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to progforperf.github.io (progforperf.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2251803 (2.1M) [application/pdf]\n",
            "Saving to: ‘/content/gdrive/MyDrive/data/pdf/c.pdf’\n",
            "\n",
            "\r          /content/   0%[                    ]       0  --.-KB/s               \r/content/gdrive/MyD 100%[===================>]   2.15M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-07-25 07:44:38 (33.8 MB/s) - ‘/content/gdrive/MyDrive/data/pdf/c.pdf’ saved [2251803/2251803]\n",
            "\n",
            "--2023-07-25 07:44:38--  https://www.halvorsen.blog/documents/programming/python/resources/Python%20Programming.pdf\n",
            "Resolving www.halvorsen.blog (www.halvorsen.blog)... 46.250.210.141\n",
            "Connecting to www.halvorsen.blog (www.halvorsen.blog)|46.250.210.141|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3463729 (3.3M) [application/pdf]\n",
            "Saving to: ‘/content/gdrive/MyDrive/data/pdf/python.pdf’\n",
            "\n",
            "/content/gdrive/MyD 100%[===================>]   3.30M  4.25MB/s    in 0.8s    \n",
            "\n",
            "2023-07-25 07:44:39 (4.25 MB/s) - ‘/content/gdrive/MyDrive/data/pdf/python.pdf’ saved [3463729/3463729]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from langchain.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
        "\n",
        "loader = DirectoryLoader('/content/gdrive/MyDrive/data/pdf/', glob=\"**/*.pdf\", loader_cls=PyMuPDFLoader)\n",
        "docs = loader.load()\n"
      ],
      "metadata": {
        "id": "w_uFMOZmUL8K"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "text_splitter\n",
        "- CharacterTextSplitter:separator > len\n",
        "- RecursiveCharacterTextSplitter: (support metadata)\n",
        "- TokenTextSplitter"
      ],
      "metadata": {
        "id": "52D7d7iqcfBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "\n",
        "# text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
        "recur_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=10)"
      ],
      "metadata": {
        "id": "GTm4k9-9bKfC"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split_docs = text_splitter.split_documents(docs)\n",
        "split_docs = recur_splitter.split_documents(docs)\n",
        "\n",
        "print(len(split_docs))\n",
        "print(len(split_docs[4].page_content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJTqxFFsc2Yg",
        "outputId": "c5e2c46b-c48b-403b-ebf9-c9511640bd8b"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "964\n",
            "927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "import pinecone"
      ],
      "metadata": {
        "id": "LND-rbhxil9q"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key=os.getenv('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "wwamNocMiwE8"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "CIraQV-6izBu"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone.init(\n",
        "    api_key=os.getenv('PINECONE_API_KEY'),  # find at app.pinecone.io\n",
        "    environment=os.getenv('PINECONE_ENV'),  # next to api key in console\n",
        ")\n",
        "\n",
        "pinecone_indexes = pinecone.list_indexes()\n",
        "if 'conversation-pdf' in pinecone_indexes:\n",
        "    print(\"do something like: pinecone.delete_index('conversation-pdf'), and re-create\")\n",
        "else:\n",
        "    pinecone.create_index('conversation-pdf', dimension=1536, metric=\"cosine\")\n",
        "db_pinecone = Pinecone.from_documents(split_docs, embeddings, index_name='conversation-pdf')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOjburB8gB9U",
        "outputId": "88523f01-5272-4beb-dd70-57b443c28a45"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "do something like: pinecone.delete_index('conversation-pdf'), and re-create\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# questions = [\n",
        "#     \"what is C programming language?\",\n",
        "#     \"what is point in C?\",\n",
        "#     \"what is decorator in Python?\",\n",
        "# ]\n",
        "\n",
        "# chat_history = []"
      ],
      "metadata": {
        "id": "PjK6eOBflurd"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# answer_docs = db_pinecone.similarity_search(question[0])\n",
        "# answer_docs"
      ],
      "metadata": {
        "id": "fVpui0duidZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "def track_token_usage(chain, query):\n",
        "    with get_openai_callback() as cb:\n",
        "        result = chain(query)\n",
        "        print(cb)\n",
        "        print(\"\\n\")\n",
        "    return result\n",
        "\n",
        "\n",
        "# def token_usage():\n",
        "#     def decorate(func):\n",
        "#         @functools.wraps(func)\n",
        "#         def wrapper(*args, **kw):\n",
        "#             result = strack_token_usage(func, *args, **kw)\n",
        "#             return result\n",
        "#         # wrapper.__name__ = func.__name__\n",
        "#         # wrapper.__doc__ = func.__doc__\n",
        "#         return wrapper\n",
        "#     return decorate"
      ],
      "metadata": {
        "id": "exM3ybSlwms6"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.retrievers import PineconeHybridSearchRetriever\n",
        "# from pinecone_text.sparse import BM25Encoder\n",
        "\n",
        "retriever = db_pinecone.as_retriever()"
      ],
      "metadata": {
        "id": "-EZO82zjmoxj"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo', verbose=False)"
      ],
      "metadata": {
        "id": "EKtSDEwGoWfZ"
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"what is C programming language?\",\n",
        "    \"what did I just ask?\",\n",
        "]\n",
        "\n",
        "chat_history = []"
      ],
      "metadata": {
        "id": "OzaXmALqv8gN"
      },
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conversation(chain, questions):\n",
        "    for question in questions:\n",
        "        input = {'question': question, 'chat_history': chat_history}\n",
        "        result = chain(input)\n",
        "        # result = track_token_usage(chain, input)\n",
        "        chat_history.append((question, result['answer']))\n",
        "        print(\"Question:\\n %s \\n\" % question)\n",
        "        print(\"Answer:\\n %s \\n\" % result['answer'])\n",
        "        print(\"---------------------------------- \\n\")"
      ],
      "metadata": {
        "id": "N4v0E8rxyhiu"
      },
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversation based on local PDF"
      ],
      "metadata": {
        "id": "oaydw7ou5NqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "                        llm=llm,\n",
        "                        retriever=retriever,\n",
        "                        verbose=True,\n",
        "                    )"
      ],
      "metadata": {
        "id": "1MxwqjJo2w_8"
      },
      "execution_count": 337,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation(conversation_chain, questions)"
      ],
      "metadata": {
        "id": "mqHPIIgmpUVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversation based on local PDF and memory\n",
        "- Open issue: https://github.com/langchain-ai/langchain/issues/2303"
      ],
      "metadata": {
        "id": "afzxtSwcQE4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Open issue: https://github.com/langchain-ai/langchain/issues/2303\n",
        "\n",
        "chat_memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "conversation_chain_buffer_memory = ConversationalRetrievalChain.from_llm(\n",
        "        llm,\n",
        "        retriever=retriever,\n",
        "        # return_source_documents=True,\n",
        "        verbose=False,\n",
        "        memory=chat_memory,\n",
        "        # output_key='answer',\n",
        "        # combine_docs_chain_kwargs={'prompt': prompt},\n",
        "        # condense_question_llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "    )\n",
        "\n",
        "\n",
        "for question in questions:\n",
        "    input = {'question': question, 'chat_history': chat_history}\n",
        "    result = conversation_chain_buffer_memory(input)\n",
        "    chat_history.append((question, result['answer']))\n",
        "    print(\"Question:\\n %s \\n\" % question)\n",
        "    print(\"Answer:\\n %s \\n\" % result['answer'])\n",
        "    print(\"---------------------------------- \\n\")\n",
        "\n",
        "\n",
        "print(conversation_chain_buffer_memory.memory.buffer)\n",
        "\n",
        "\n",
        "# transform = TransformChain(\n",
        "#     input_variables=[\"input\"],\n",
        "#     output_variables=[\"question\", \"chat_history\"],\n",
        "#     transform = lambda inputs: {\n",
        "#         \"question\": inputs[\"input\"],\n",
        "#         \"chat_history\": [\n",
        "#             (human.group(1).strip(), ai.group(1).strip())\n",
        "#             for human, ai in zip(\n",
        "#                 re.compile(r'Human: (.*(?:\\n(?!(Human|AI):).*)*)').finditer(inputs[\"history\"]),\n",
        "#                 re.compile(r'AI: (.*(?:\\n(?!(Human|AI):).*)*)').finditer(inputs[\"history\"]),\n",
        "#             )\n",
        "#         ],\n",
        "#     },\n",
        "# )\n",
        "\n",
        "# conversational_retrieval = ConversationalRetrievalChain.from_llm(\n",
        "#     llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0),\n",
        "#     retriever=retriever,\n",
        "# )\n",
        "\n",
        "# chat = SequentialChain(\n",
        "#     memory=ConversationBufferMemory(),\n",
        "#     input_variables=[\"input\"],\n",
        "#     output_variables=[\"answer\"],\n",
        "#     chains=[transform, conversational_retrieval],\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "# for question in questions:\n",
        "#     input = {'question': question, 'chat_history': chat_history}\n",
        "#     result = chat.run(question)\n",
        "#     # chat_history.append((question, result['answer']))\n",
        "#     print(\"Question:\\n %s \\n\" % question)\n",
        "#     print(\"Answer:\\n %s \\n\" % result)\n",
        "#     print(\"---------------------------------- \\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8_lZa8e2vbs",
        "outputId": "44a76922-4278-4b1d-c6df-419dec97ed0e"
      },
      "execution_count": 339,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:\n",
            " what is C programming language? \n",
            "\n",
            "Answer:\n",
            " C programming language is a high-level programming language that was developed in the early 1970s. It was originally designed for system programming and has since become one of the most widely used programming languages. C is known for its simplicity, efficiency, and flexibility, making it suitable for a wide range of applications. It allows programmers to have direct control over the hardware and memory, making it ideal for developing operating systems, embedded systems, and other low-level applications. C has influenced many other programming languages and is often used as a foundation for learning other languages. \n",
            "\n",
            "---------------------------------- \n",
            "\n",
            "Question:\n",
            " what did I just ask? \n",
            "\n",
            "Answer:\n",
            " I'm sorry, but I don't have access to the previous question. \n",
            "\n",
            "---------------------------------- \n",
            "\n",
            "[HumanMessage(content='what is C programming language?', additional_kwargs={}, example=False), AIMessage(content='C programming language is a high-level programming language that was developed in the early 1970s. It was originally designed for system programming and has since become one of the most widely used programming languages. C is known for its simplicity, efficiency, and flexibility, making it suitable for a wide range of applications. It allows programmers to have direct control over the hardware and memory, making it ideal for developing operating systems, embedded systems, and other low-level applications. C has influenced many other programming languages and is often used as a foundation for learning other languages.', additional_kwargs={}, example=False), HumanMessage(content='what did I just ask?', additional_kwargs={}, example=False), AIMessage(content=\"I'm sorry, but I don't have access to the previous question.\", additional_kwargs={}, example=False)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversation with local PDF and summary\n",
        "- havn't supportted"
      ],
      "metadata": {
        "id": "tWKcOvOrZP2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversation cache"
      ],
      "metadata": {
        "id": "nWwj4x-1bCgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Open issue: https://github.com/langchain-ai/langchain/issues/2303\n",
        "\n",
        "chat_memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "conversation_chain_buffer_memory = ConversationalRetrievalChain.from_llm(\n",
        "        llm,\n",
        "        retriever=retriever,\n",
        "        # return_source_documents=True,\n",
        "        verbose=False,\n",
        "        memory=chat_memory,\n",
        "        # output_key='answer',\n",
        "        # combine_docs_chain_kwargs={'prompt': prompt},\n",
        "        # condense_question_llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "    )\n",
        "\n",
        "\n",
        "for question in questions:\n",
        "    input = {'question': question, 'chat_history': chat_history}\n",
        "    result = conversation_chain_buffer_memory(input)\n",
        "    chat_history.append((question, result['answer']))\n",
        "    print(\"Question:\\n %s \\n\" % question)\n",
        "    print(\"Answer:\\n %s \\n\" % result['answer'])\n",
        "    print(\"---------------------------------- \\n\")\n",
        "\n",
        "\n",
        "print(conversation_chain_buffer_memory.memory.buffer)\n"
      ],
      "metadata": {
        "id": "AbvA0To4bBfh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}